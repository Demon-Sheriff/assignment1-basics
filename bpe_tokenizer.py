# -*- coding: utf-8 -*-
"""bpe-tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1omsi3yl85kZLo-1KSxpwqXUWLSF7tbfx
"""

import torch
import numpy as np
import os
import regex as re
from collections import defaultdict
from multiprocessing import Pool
from typing import BinaryIO

torch.manual_seed(341)
np.random.seed(341)

print(ord('牛')); print(chr(29275))

print(chr(0)); chr(0)

"this is a test string" + chr(0) + "string"

print("this is a test string" + chr(0) + "string")

test_string = "hello! こんにちは!"
utf8_encoded = test_string.encode('utf-8') # byte string

print(utf8_encoded)

print(type(utf8_encoded))

"".join([chr(o) for o in list(utf8_encoded)])

print(len(test_string)); print(len(utf8_encoded)); utf8_encoded.decode('utf-8')

def decode_utf8_bytes_to_str(bytestring: bytes) -> str:
  return bytestring.decode('utf-8')

decode_utf8_bytes_to_str("hello! こんにちは!".encode('utf-8'))

PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

re.findall(PAT, "some text that i'll pre-tokenize!")

test = "don't pre-tokenize this, okay?"
pre_tokens = re.findall(PAT, test)
byte_sequences = [list(token.encode('utf-8')) for token in pre_tokens]

PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
TOKENIZER = re.compile(PAT)
SPECIAL_TOKEN = "<|endoftext|>"

def find_chunk_boundaries(file: BinaryIO, desired_num_chunks: int, split_special_token: bytes) -> list[int]:
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)

    chunk_size = file_size // desired_num_chunks
    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]
    chunk_boundaries[-1] = file_size

    mini_chunk_size = 4096
    for bi in range(1, len(chunk_boundaries) - 1):
        pos = chunk_boundaries[bi]
        file.seek(pos)
        while True:
            mini_chunk = file.read(mini_chunk_size)
            if mini_chunk == b"":
                chunk_boundaries[bi] = file_size
                break
            found_at = mini_chunk.find(split_special_token)
            if found_at != -1:
                chunk_boundaries[bi] = pos + found_at
                break
            pos += mini_chunk_size
    return sorted(set(chunk_boundaries))

import os
from typing import BinaryIO

# Function to find Find safe chunk boundaries
def find_chunk_boundaries(
    file: BinaryIO,
    desired_num_chunks: int,
    split_special_token: bytes
) -> list[int]:
    """
    Chunk the file into parts that can be counted independently.
    May return fewer chunks if the boundaries end up overlapping.
    """
    assert isinstance(split_special_token, bytes), (
        "Must represent special token as a bytestring"
    )

    # Get total file size in bytes
    file.seek(0, os.SEEK_END)
    file_size = file.tell()
    file.seek(0)

    chunk_size = file_size // desired_num_chunks

    # Initial guesses for chunk boundary locations, uniformly spaced
    # Chunks start on previous index, don't include last index
    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]
    chunk_boundaries[-1] = file_size

    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time

    for bi in range(1, len(chunk_boundaries) - 1):
        initial_position = chunk_boundaries[bi]
        file.seek(initial_position)  # Start at boundary guess
        while True:
            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk

            # If EOF, this boundary should be at the end of the file
            if mini_chunk == b"":
                chunk_boundaries[bi] = file_size
                break

            # Find the special token in the mini chunk
            found_at = mini_chunk.find(split_special_token)
            if found_at != -1:
                chunk_boundaries[bi] = initial_position + found_at
                break
            initial_position += mini_chunk_size

    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks
    return sorted(set(chunk_boundaries))

!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

# Worker function to process chunks
def process_chunk(args):
    filename, start, end = args
    counts = defaultdict(int)

    with open(filename, "rb") as f:
        f.seek(start)
        chunk = f.read(end - start).decode("utf-8", errors="ignore")

    docs = re.split(re.escape(SPECIAL_TOKEN), chunk)
    for doc in docs:
        for match in TOKENIZER.finditer(doc):
            token = match.group()
            counts[token] += 1

    return counts

# reducer function
def merge_counts(dicts):
    merged = defaultdict(int)
    for d in dicts:
        for k, v in d.items():
            merged[k] += v
    return merged

# parallel pre tokenization function
def run_pre_tokenization_parallel(filename: str, num_processes: int = 4):
    with open(filename, "rb") as f:
        boundaries = find_chunk_boundaries(f, num_processes, SPECIAL_TOKEN.encode("utf-8"))

    args = [(filename, s, e) for s, e in zip(boundaries[:-1], boundaries[1:])]

    with Pool(num_processes) as pool:
        partial_counts = pool.map(process_chunk, args)

    full_counts = merge_counts(partial_counts)
    return full_counts

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = re.compile(PAT)
# 
# pre_tokens = []
# 
# # serial impl
# with open("/content/TinyStoriesV2-GPT4-valid.txt", "rb") as f:
#     boundaries = find_chunk_boundaries(f, desired_num_chunks=4, split_special_token=b"<|endoftext|>")
# 
#     for start, end in zip(boundaries[:-1], boundaries[1:]):
#         f.seek(start)
#         chunk = f.read(end - start).decode("utf-8", errors="ignore")
#         pre_tokens += tokenizer.findall(chunk)  # regex pre-tokenization

len(pre_tokens), len(boundaries)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pre_token_counts = run_pre_tokenization_parallel("/content/TinyStoriesV2-GPT4-valid.txt", num_processes=5)
# print(sorted(pre_token_counts.items(), key=lambda x: -x[1])[:10])  # Top 10 tokens

for k in pre_token_counts:
  if k[0] in ['|', '<', '>', ':', ]:
    print(k)















def bytes_to_unicode():
    """
    Returns a dict mapping byte values (0–255) to unicode strings,
    to ensure reversibility and printable output.
    """
    bs = list(range(ord("!"), ord("~") + 1)) + \
         list(range(ord("¡"), ord("¬") + 1)) + \
         list(range(ord("®"), ord("ÿ") + 1))
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    return dict(zip(bs, [chr(c) for c in cs]))

b2u = bytes_to_unicode()
byte_sequences_vis = [
    [b2u[b] for b in token.encode("utf-8")] for token in pre_tokens
]

byte_sequences

type(byte_sequences[0][0])

import heapq

def get_pairs(byte_sequences: list[list[int]]) -> dict[tuple[int, int], int]:
  pair_freq_dict = {}
  for seq in byte_sequences:
      for i in range(len(seq) - 1):
          pair = (seq[i], seq[i + 1])
          pair_freq_dict[pair] = pair_freq_dict.get(pair, 0) + 1

  return pair_freq_dict

pairs = get_pairs(byte_sequences)
sorted_pairs = sorted(pairs.items(), key=lambda x: (-x[1], x[0]))

sorted_pairs[0]

sorted_pairs

def train(pairs):
  sorted_pairs = sorted(pairs.items(), key=lambda x: (-x[1], x[0]))
  top = sorted_pairs[0]  # highest freq pair


  # highest freq







def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) ->
              dict[int, bytes], list[tuple[bytes, bytes]]:

              pass